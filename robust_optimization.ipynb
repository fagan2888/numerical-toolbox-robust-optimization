{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributionally robust optimization\n",
    "\n",
    "This notebooks collects our exploration for the application of results from distributionally robust optimization (DRO) to calculte the worst-case expected maximum future value (EMAX).\n",
    "\n",
    "The key reference at this point is the following:\n",
    "\n",
    "* Zhaolin Hu, L. Jeff Hong (2013). [Kullback-Leibler divergence constrained distributionally robust optimizations](http://personal.cb.cityu.edu.hk/jeffhong/Papers/HuHong2013_technicalreport.pdf), technical report. \n",
    "\n",
    "The key result in this paper is **Theorem 1** where they establish that we can determine the worst-case value if the uncertainty set is the Kullback-Leibler divergence by simply solving the one-dimensional optimization problem.\n",
    "\n",
    "\\begin{align*}\n",
    "\\min_{\\alpha \\geq 0} \\alpha \\log E_{P_0}\\left[\\exp^{H(x, \\xi) / \\alpha}\\right] + \\alpha \\eta\n",
    "\\end{align*}\n",
    "\n",
    "**Structure**\n",
    "\n",
    "* Example: Linear case with normal distirbution\n",
    "\n",
    "* Comparison to discretization approach\n",
    "\n",
    "* Comparing optimization algorithms\n",
    "\n",
    "**TODO**\n",
    "\n",
    "* We want to experiment with using IPOPT for the optimization.\n",
    "\n",
    "* We want to improve the setup for the discretization approach which at first glance might actually be quite competitive in our setting where the evaluation of the expectation in the optimization step is quite costly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Linear case with normal distribution\n",
    "\n",
    "This code reproduces the example in the above manuscript presented in Section 5.2. Our application differs from this toy example only in the structure of the objective function $H(x, \\xi)$. Here it is assumed to be linear $H(x, \\xi) = \\xi^T x$ whereas it is nonlinear in our case where  $H(x, \\xi) = \\max\\{V_1(x, \\xi_1), V_2(x, \\xi_2), V_3(x, \\xi_3), V_4(x, \\xi_4), V_5(x, \\xi_5)\\}$.\n",
    "\n",
    "This example has some nice feature for developing our production code:\n",
    "\n",
    "* We can simply exchange the criterion function with our version.\n",
    "\n",
    "* We can easily experiment with differnt problem dimsions.\n",
    "\n",
    "* We can compare the performance of our numerical solution with the closed form solution. \n",
    "\n",
    "### Insights\n",
    "\n",
    "* The quality of the numerical integration of the EMAX function is key in the performance (number of function evaluations required for the optimization) and quality (comparison to the closed-form solution).\n",
    "\n",
    "* The one-dimensional optimization is very robust to choosing alternative optimization algorithms.\n",
    "\n",
    "* We need to focus all effort on improving the quality of the numerical integration.\n",
    "\n",
    "### Remarks\n",
    "\n",
    "* The code below allows to investigate the impact of using Monte Carlo integration for the evaluation of the expectation as we also can simply comment in the closed-from solution for this ingredient into the optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2.2513399839578927, 0.4672106648335468, 0, 28)\n",
      "0.4472135954999579\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "from scipy.optimize import fminbound\n",
    "import numpy as np\n",
    "\n",
    "EPS_FLOAT = np.finfo(1.0).eps\n",
    "\n",
    "    \n",
    "def objective_function(x, eps):\n",
    "    return np.dot(eps, x)\n",
    "    \n",
    "def evaluate_expectation_mc(model_spec, *args):\n",
    "    def integrand(model_spec, alpha, eps):\n",
    "        return np.exp(objective_function(model_spec['x'], eps) / alpha)\n",
    "    alpha, eps_draws = args\n",
    "    p_integrand = partial(integrand, model_spec, alpha)\n",
    "    return np.mean(np.apply_along_axis(p_integrand, 1, eps_draws))\n",
    "\n",
    "def evaluate_expectation_cf(model_spec, *args):\n",
    "    alpha = args[0]\n",
    "    arg1 = np.dot(model_spec['mean'], model_spec['x']) / alpha\n",
    "    arg2 = np.dot(np.dot(model_spec['x'], model_spec['covs']), model_spec['x']) / (2 * alpha ** 2 )\n",
    "    return np.exp(arg1 + arg2)\n",
    "        \n",
    "def convex_function(model_spec, eps_draws, alpha):\n",
    "    expectation = evaluate_expectation(model_spec, alpha, eps_draws)\n",
    "    rslt = alpha * np.log(expectation) + alpha * model_spec['eta']\n",
    "    return rslt\n",
    "\n",
    "def closed_form_solution(model_spec):\n",
    "    arg1 = np.dot(model_spec['mean'], model_spec['x'])\n",
    "    arg2 = np.sqrt(2.0 * model_spec['eta'])\n",
    "    arg3 = np.sqrt(np.dot(np.dot(model_spec['x'], model_spec['covs']), model_spec['x']))\n",
    "    return arg1 + arg2 * arg3\n",
    "\n",
    "def get_worst_case_expectation(model_spec):\n",
    "    \"\"\"This is the wrapper\"\"\"\n",
    "    np.random.seed(21)\n",
    "    eps_draws = np.random.multivariate_normal(model_spec['mean'], model_spec['covs'], model_spec['num_draws'])\n",
    "\n",
    "    lower, upper = 0.001, 5000\n",
    "    rslt = fminbound(partial(convex_function, model_spec, eps_draws), lower, upper, xtol=EPS_FLOAT, full_output=True)\n",
    "\n",
    "    return rslt\n",
    "\n",
    "\n",
    "model_spec = dict()\n",
    "model_spec['x'] = np.ones(1)\n",
    "model_spec['mean'] = np.zeros(1)\n",
    "model_spec['covs'] = np.identity(1)\n",
    "model_spec['num_draws'] = 10000\n",
    "model_spec['eta'] = 0.1\n",
    "\n",
    "# We can use this code to either use the closed-form solution for the integration \n",
    "# of the expectation or do so by Monte Carlo integration. This allows to get a sense\n",
    "# of how the simulation error affects the optimization step.\n",
    "is_closed = False\n",
    "if is_closed:\n",
    "    evaluate_expectation = evaluate_expectation_cf\n",
    "else:\n",
    "    evaluate_expectation = evaluate_expectation_mc\n",
    "\n",
    "print(get_worst_case_expectation(model_spec))\n",
    "print(closed_form_solution(model_spec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison to discretization approach\n",
    "\n",
    "We want to compare the results from the more general approach above to the discretization approach we initially pursued. We will illustrate this the simple example of univariate standard normal distribution.\n",
    "\n",
    "### Insights\n",
    "\n",
    "* The results are rather close and the discretization approach is quite fast as it does not require to repeatedly solve the EMAX integral as in the approach above. We need to keep this in mind for some more extensive benchmarking at a later point. \n",
    "\n",
    "The key reference for this approach is:\n",
    "\n",
    "* Arnab Nilim, Laurent El Ghaoui (2003). [Robust Control of Markov Decision Processes with\n",
    "Uncertain Transition Matrices](https://people.eecs.berkeley.edu/~elghaoui/Pubs/RobMDP_OR2005.pdf), Operations Research, Vol. 53, No. 5, p.780 - 798.\n",
    "\n",
    "The algorithm for discrete transitions is presented in Section 6.1 and implemented in the **robupy** package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from robupy.auxiliary import get_worst_case_probs\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_worst_case_discretization(model_spec, num_points= 100000):\n",
    "    np.random.seed(123)\n",
    "    v = norm.rvs(size=num_points)\n",
    "\n",
    "    q = np.tile(1.0, num_points) \n",
    "    q = q / np.sum(q)\n",
    "\n",
    "    eta = model_spec['eta']\n",
    "    p = get_worst_case_probs(v, q, eta)\n",
    "    return np.matmul(v, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can compare for a standard case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_spec = dict()\n",
    "model_spec['x'] = np.ones(1)\n",
    "model_spec['mean'] = np.zeros(1)\n",
    "model_spec['covs'] = np.identity(1)\n",
    "model_spec['num_draws'] = 1000\n",
    "model_spec['eta'] = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how well the discretization performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "discetization approach: 0.448\n",
      "closed form: 0.447\n"
     ]
    }
   ],
   "source": [
    "print('discetization approach: {:5.3f}'.format(get_worst_case_discretization(model_spec)))\n",
    "print('closed form: {:5.3f}'.format(closed_form_solution(model_spec)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing optimization algorihtms\n",
    "\n",
    "We compare different optimization algorithms to check for any differences in performance.\n",
    "\n",
    "### Insights\n",
    "\n",
    "* All algorithms pretty much find the same points but the derivative-based algorithsm do so with much less function evaluations. This appears true even though we only provide a noisy deriative that includes numerical and simulation error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COBYLA (Constrained Optimization BY Linear Approximations) (local, no-derivative)                           2.25129   0.46721   27    4\n",
      "Principal-axis, praxis (local, no-derivative)                                                               2.25134   0.46721  100    5\n",
      "Nelder-Mead simplex algorithm (local, no-derivative)                                                        2.25140   0.46721   30    4\n",
      "Sbplx variant of Nelder-Mead (re-implementation of Rowan's Subplex) (local, no-derivative)                  2.25136   0.46721   49    4\n",
      "Method of Moving Asymptotes (MMA) (local, derivative)                                                       2.25133   0.46721   13    4\n",
      "Sequential Quadratic Programming (SQP) (local, derivative)                                                  2.25134   0.46721   10    4\n",
      "Limited-memory BFGS (L-BFGS) (local, derivative-based)                                                      2.25134   0.46721   18    4\n",
      "Preconditioned truncated Newton with restarting (local, derivative-based)                                   2.25133   0.46721   12    1\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import approx_fprime\n",
    "import nlopt\n",
    "\n",
    "ALGO_LOCAL_LD = [nlopt.LD_MMA, nlopt.LD_SLSQP, nlopt.LD_LBFGS, nlopt.LD_TNEWTON_PRECOND_RESTART]\n",
    "ALGO_LOCAL_LN = [nlopt.LN_COBYLA, nlopt.LN_PRAXIS, nlopt.LN_NELDERMEAD, nlopt.LN_SBPLX]\n",
    "\n",
    "# Parameters for optimization\n",
    "APPROX_FPRIME_EPS = 10e-6\n",
    "NLOPT_XTOL_REL = 10e-5\n",
    "NLOPT_MAXEVAL = 100\n",
    "\n",
    "model_spec = dict()\n",
    "model_spec['x'] = np.ones(1)\n",
    "model_spec['mean'] = np.zeros(1)\n",
    "model_spec['covs'] = np.identity(1)\n",
    "model_spec['num_draws'] = 10000\n",
    "model_spec['eta'] = 0.1\n",
    "\n",
    "def convex_function_nlopt(model_spec, eps_draws, alpha, grad=np.array([])):\n",
    "    if grad.size > 0:\n",
    "        grad[0]  = approx_fprime(alpha, test_func, APPROX_FPRIME_EPS)\n",
    "    expectation = evaluate_expectation(model_spec, alpha, eps_draws)\n",
    "    rslt = alpha * np.log(expectation) + alpha * model_spec['eta']\n",
    "    return float(rslt)\n",
    "\n",
    "np.random.seed(21)\n",
    "eps_draws = np.random.multivariate_normal(model_spec['mean'], model_spec['covs'], model_spec['num_draws'])\n",
    "test_func = partial(convex_function_nlopt, model_spec, eps_draws)\n",
    "\n",
    "for algo in ALGO_LOCAL_LN + ALGO_LOCAL_LD:\n",
    "    \n",
    "    opt = nlopt.opt(algo, 1)\n",
    "    opt.set_lower_bounds([EPS_FLOAT])\n",
    "    opt.set_min_objective(test_func)\n",
    "    opt.set_xtol_rel(NLOPT_XTOL_REL)\n",
    "    opt.set_maxeval(NLOPT_MAXEVAL)\n",
    "    x = opt.optimize([0.5])\n",
    "    \n",
    "    info = list()\n",
    "    info += [opt.get_algorithm_name(), x[0], opt.last_optimum_value()]\n",
    "    info += [opt.get_numevals(), opt.last_optimize_result()]\n",
    "    print('{:<105}{:10.5f}{:10.5f}{:5d}{:5d}'.format(*info))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
